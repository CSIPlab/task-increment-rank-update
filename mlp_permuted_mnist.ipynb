{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c707d9",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb9bca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from glob import glob\n",
    "from scipy import misc\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "from torch import nn\n",
    "from os.path import isfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform\n",
    "from scipy import linalg\n",
    "import scipy.io as sio\n",
    "import time\n",
    "import copy\n",
    "\n",
    "#from skimage.measure import compare_ssim as ssim # Old Version\n",
    "from skimage.metrics import structural_similarity as ssim # New Version\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from tqdm import tqdm\n",
    "from tqdm import notebook as tq\n",
    "\n",
    "device_id=0\n",
    "torch.cuda.set_device(device_id)\n",
    "device='cuda:'+str(device_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74c8d31",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b7ed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset:\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train=x_train.reshape(x_train.shape[0],-1)\n",
    "x_test=x_test.reshape(x_test.shape[0],-1)\n",
    "\n",
    "\n",
    "# Permuted MNIST\n",
    "num_task=20\n",
    "step=100\n",
    "init=100\n",
    "\n",
    "seeds=np.arange(init,num_task*step+init,step)\n",
    "n_class=len(np.unique(y_train))\n",
    "train_y=np.zeros((len(y_train),n_class))\n",
    "for i in range(train_y.shape[0]):\n",
    "    train_y[i,y_train[i]]=1\n",
    "test_y=np.zeros((len(y_test),n_class))\n",
    "for i in range(test_y.shape[0]):\n",
    "    test_y[i,y_test[i]]=1\n",
    "    \n",
    "data=[[x_train,train_y]]\n",
    "data_test=[[x_test,test_y]]\n",
    "\n",
    "pixels=np.arange(x_train.shape[1])\n",
    "\n",
    "pixel_set=[pixels]\n",
    "for task in range(num_task-1):\n",
    "    np.random.seed(seeds[task])\n",
    "    np.random.shuffle(pixels)\n",
    "    pixel_set.append(pixels)\n",
    "    \n",
    "    \n",
    "    x=x_train[:,pixels]    \n",
    "    data.append([x,train_y])\n",
    "    \n",
    "    x=x_test[:,pixels]    \n",
    "    data_test.append([x,test_y])\n",
    "# Parameters:\n",
    "\n",
    "NUM_K_FOLDS = 1 # if > 1, parition the dataset into K held-out partitions\n",
    "NUM_FOLDS_VALIDATION = 1 # Train and test on this many partitions; must be smaller than NUM_K_FOLDS\n",
    "# Use NUM_K_FOLDS = 1 to train and test on the entire dataset set.\n",
    "\n",
    "# Partition dataset into \n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test = [], []\n",
    "Y_train, Y_test = [], []\n",
    "# NUM_K_FOLDS partitons:\n",
    "if NUM_K_FOLDS >= 2:\n",
    "    kfold = KFold(n_splits=NUM_K_FOLDS, shuffle=True)\n",
    "    for task in range(num_task):\n",
    "        X_currTask_train=data[task][0]\n",
    "        Y_currTask_train=data[task][1]\n",
    "        X_currTask_test=data_test[task][0]\n",
    "        Y_currTask_test=data_test[task][1]\n",
    "        for folds, (train_index, test_index) in enumerate(kfold.split(Y_currTask)):\n",
    "            X_train.append(X_currTask_train[train_index]) # Need to revise for reassignment\n",
    "            X_test.append(X_currTask_test[test_index]) # Need to revise for reassignment\n",
    "            Y_train.append(Y_currTask_train[train_index]) # Need to revise for reassignment\n",
    "            Y_test.append(Y_currTask_test[test_index]) # Need to revise for reassignment\n",
    "# Full dataset:\n",
    "else:\n",
    "    for task in range(num_task):\n",
    "        X_currTask_train=data[task][0]\n",
    "        Y_currTask_train=data[task][1]\n",
    "        X_currTask_test=data_test[task][0]\n",
    "        Y_currTask_test=data_test[task][1]\n",
    "        \n",
    "        X_train.append(X_currTask_train) # Need to revise for reassignment\n",
    "        X_test.append(X_currTask_test) # Need to revise for reassignment\n",
    "        Y_train.append(Y_currTask_train) # Need to revise for reassignment\n",
    "        Y_test.append(Y_currTask_test) # Need to revise for reassignment\n",
    "X_train = np.asarray(X_train)\n",
    "X_train = np.reshape(X_train,(num_task,NUM_K_FOLDS,X_train.shape[1],X_train.shape[2])) # tasks X folds\n",
    "X_test = np.asarray(X_test)\n",
    "X_test = np.reshape(X_test,(num_task,NUM_K_FOLDS,X_test.shape[1],X_test.shape[2])) # tasks X folds\n",
    "Y_train = np.asarray(Y_train)\n",
    "Y_train = np.reshape(Y_train,(num_task,NUM_K_FOLDS,Y_train.shape[1],Y_train.shape[2])) # tasks X folds\n",
    "Y_test = np.asarray(Y_test)\n",
    "Y_test = np.reshape(Y_test,(num_task,NUM_K_FOLDS,Y_test.shape[1],Y_test.shape[2])) # tasks X folds\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bf4d6c",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b277da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "num_task=20\n",
    "n_class=10 # Ouput dimension\n",
    "\n",
    "# Selection of r1, r2:\n",
    "r1_byfolds = [11]\n",
    "r2_byfolds = [1]\n",
    "\n",
    "batch_size=128\n",
    "# Epochs by task (task1, task2, task3 ....):\n",
    "#train_epochs=[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]\n",
    "train_epochs=[500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500]\n",
    "\n",
    "lr_base = 1e-5\n",
    "lr_cont = 1e-4\n",
    "lr_drop_epoch = []\n",
    "lr_drop_factor = 0.5\n",
    "\n",
    "# Layers def:\n",
    "relu = nn.ReLU()\n",
    "sigmoid=nn.Sigmoid()\n",
    "softmax=torch.nn.Softmax(dim=1)\n",
    "Lsoftmax = nn.LogSoftmax(dim=1)\n",
    "mse_loss = nn.MSELoss(reduction='mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f002f911",
   "metadata": {},
   "source": [
    "# Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed02d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two factorized intermediate layers FCN(with selector):\n",
    "# Use K-fold for parameter sweep if needed.\n",
    "# fw1, fw2 are concatenated instead of added.\n",
    "\n",
    "# Parameters:\n",
    "\n",
    "\n",
    "w_task=[]\n",
    "\n",
    "# Train test sequence:\n",
    "accuracy = []\n",
    "for fold in range(NUM_FOLDS_VALIDATION):\n",
    "    fold_accuracy = []\n",
    "    if NUM_K_FOLDS <= 1:\n",
    "        print(\"Full Training Set(no partition):\")\n",
    "    else:\n",
    "        print(\"Folds Number \", fold, \"/\",NUM_FOLDS_VALIDATION)\n",
    "    print(\"Epochs:\",train_epochs[fold], \"Batch Size:\", batch_size,\"R1:\",r1_byfolds[fold],\"R2:\",r2_byfolds[fold])\n",
    "    \n",
    "    # Reset model every folds:\n",
    "    # Intialize Weights:\n",
    "    for task in range(num_task):\n",
    "        r1 = r1_byfolds[fold]\n",
    "        r2 = r2_byfolds[fold]\n",
    "        if task==0:\n",
    "            r=r1\n",
    "            lr = lr_base\n",
    "        else:\n",
    "            r=r2\n",
    "            lr = lr_cont\n",
    "            \n",
    "        \n",
    "        \n",
    "        wR1 = torch.empty(x_train.shape[1], r).to(device)\n",
    "        wL1 = torch.empty(256, r).to(device)\n",
    "        s1 = torch.ones(r).to(device)\n",
    "        b1 = torch.zeros(256).to(device)\n",
    "        \n",
    "        wR2 = torch.empty(256, r).to(device)\n",
    "        wL2 = torch.empty(256, r).to(device)\n",
    "        s2 = torch.ones(r).to(device)\n",
    "        b2 = torch.zeros(256).to(device)\n",
    "        \n",
    "        w3=torch.empty(256, n_class).to(device)\n",
    "        b3 = torch.zeros(n_class).to(device)\n",
    "        \n",
    "        # Initialization:\n",
    "        nn.init.orthogonal_(wR1)\n",
    "        nn.init.orthogonal_(wL1)\n",
    "        nn.init.orthogonal_(wR2)\n",
    "        nn.init.orthogonal_(wL2)\n",
    "        nn.init.orthogonal_(w3)\n",
    "    \n",
    "        w_task.append([wR1,wL1,wR2,wL2,w3,s1,s2,b1,b2,b3])\n",
    "        \n",
    "    # Train-Test sequence:\n",
    "    print(\"Task Progress:\")\n",
    "    # Train\n",
    "    U_, S_, V_ = [],[],[]\n",
    "    U_.append(w_task[0][0]); U_.append(w_task[0][2]) # U_[n], n-th interm layer.\n",
    "    S_.append(w_task[0][5]); S_.append(w_task[0][6]) # s0\n",
    "    V_.append(w_task[0][1]); V_.append(w_task[0][3]) # wL0\n",
    "    S1_tasks = []\n",
    "    S2_tasks = []\n",
    "    b_tasks = []\n",
    "    w3_tasks = []\n",
    "    for task in tq.tqdm(range(num_task)):\n",
    "\n",
    "        dwR1=torch.autograd.Variable(w_task[task][0],requires_grad=True)\n",
    "        dwL1=torch.autograd.Variable(w_task[task][1],requires_grad=True)\n",
    "        dwR2=torch.autograd.Variable(w_task[task][2],requires_grad=True)\n",
    "        dwL2=torch.autograd.Variable(w_task[task][3],requires_grad=True)    \n",
    "        w3=torch.autograd.Variable(w_task[task][4],requires_grad=True)\n",
    "        # Selector\n",
    "        if task == 0:\n",
    "            ds1 = torch.autograd.Variable(w_task[task][5],requires_grad=True)\n",
    "            ds2 = torch.autograd.Variable(w_task[task][6],requires_grad=True)\n",
    "        # set previous task matrix as zero    \n",
    "        else:\n",
    "            #ds1 = torch.cat((S1_tasks[task-1].to(device), w_task[task][5]), dim = 0)\n",
    "            #ds2 = torch.cat((S2_tasks[task-1].to(device), w_task[task][6]), dim = 0)    \n",
    "            ds1 = torch.cat((torch.zeros(S1_tasks[task-1].shape).to(device), w_task[task][5]), dim = 0) \n",
    "            ds2 = torch.cat((torch.zeros(S2_tasks[task-1].shape).to(device), w_task[task][6]), dim = 0)      \n",
    "            ds1=torch.autograd.Variable(ds1,requires_grad=True)\n",
    "            ds2=torch.autograd.Variable(ds2,requires_grad=True)\n",
    "        # Bias\n",
    "        b1=torch.autograd.Variable(w_task[task][7],requires_grad=True)\n",
    "        b2=torch.autograd.Variable(w_task[task][8],requires_grad=True)\n",
    "        b3=torch.autograd.Variable(w_task[task][9],requires_grad=True)\n",
    "        \n",
    "        optimizer=torch.optim.Adam([dwR1,dwL1,dwR2,dwL2,w3,ds1,ds2,b1,b2,b3],lr=lr)\n",
    "        #optimizer=torch.optim.SGD([dwR1,dwL1,dwR2,dwL2,w3,ds1,ds2,b1,b2,b3],lr=lr,momentum = 0.9)\n",
    "        \n",
    "        loss_epoch=[]\n",
    "        \n",
    "        epoch_correct, epoch_total = 0, 0\n",
    "        epoch_accuracy = []\n",
    "        times = 1\n",
    "        for epoch in tq.tqdm(range(train_epochs[task])):\n",
    "            train_size=len(X_train[task][fold])\n",
    "            batch_no=np.int16(np.ceil(train_size/np.float64(batch_size)))\n",
    "        \n",
    "            epoch_idx=np.arange(train_size)\n",
    "            np.random.shuffle(epoch_idx)\n",
    "            \n",
    "            # Reduce lr during training if needed:\n",
    "            if epoch in lr_drop_epoch:\n",
    "                new_lr = lr * (lr_drop_factor ** times)\n",
    "                times = times + 1\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = new_lr\n",
    "                print(param_group['lr'])\n",
    "        \n",
    "            loss_batch = 0\n",
    "            for batch_idx in range(0, batch_no):\n",
    "                x_batch=X_train[task][fold][epoch_idx[batch_idx*batch_size:np.min([(batch_idx+1)*batch_size,train_size])],:]\n",
    "                y_batch=Y_train[task][fold][epoch_idx[batch_idx*batch_size:np.min([(batch_idx+1)*batch_size,train_size])]]\n",
    "            \n",
    "                x_batch_tensor=torch.FloatTensor(x_batch).to(device)\n",
    "                y_batch_tensor=torch.FloatTensor(y_batch).to(device)\n",
    "                \n",
    "                if task == 0:\n",
    "                    w1=torch.matmul(torch.matmul(dwR1,torch.diag(ds1)),dwL1.T)#+b1\n",
    "                    w2=torch.matmul(torch.matmul(dwR2,torch.diag(ds2)),dwL2.T)#+b2\n",
    "                else:\n",
    "                    U_inc1 = torch.cat((U_[0].to(device), dwR1), dim = 1)\n",
    "                    V_inc1 = torch.cat((V_[0].to(device), dwL1), dim = 1)\n",
    "                    w1=torch.matmul(torch.matmul(U_inc1,torch.diag(ds1)),V_inc1.T)#+b1\n",
    "                    \n",
    "                    U_inc2 = torch.cat((U_[1].to(device), dwR2), dim = 1)\n",
    "                    V_inc2 = torch.cat((V_[1].to(device), dwL2), dim = 1)\n",
    "                    w2=torch.matmul(torch.matmul(U_inc2,torch.diag(ds2)),V_inc2.T)#+b2\n",
    "                    \n",
    "                h1=relu(torch.matmul(x_batch_tensor,w1)+b1)\n",
    "                h2=relu(torch.matmul(h1,w2)+b2)\n",
    "                \n",
    "                y_hat=softmax(torch.matmul(h2,w3)+b3)\n",
    "                \n",
    "                loss=mse_loss(y_hat,y_batch_tensor)\n",
    "                \n",
    "                loss_batch = loss_batch + loss.item()\n",
    "                \n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "            \n",
    "                optimizer.step()\n",
    "                \n",
    "                # Prediction:\n",
    "                prediction = np.argmax(y_hat.cpu().detach().numpy(),axis=1)\n",
    "                y_label=np.argmax(y_batch_tensor.cpu().detach().numpy(),axis=1)\n",
    "                epoch_correct += int(sum(y_label == prediction))\n",
    "                epoch_total += int(len(y_label))\n",
    "            \n",
    "            epoch_accuracy.append(float(epoch_correct / epoch_total))\n",
    "            epoch_correct, epoch_total = 0, 0\n",
    "                \n",
    "            loss_epoch.append(loss_batch)\n",
    "            \n",
    "        # Concatenate the weight matrix of every tasks:\n",
    "        with torch.no_grad():\n",
    "            if task == 0:\n",
    "                U_[0] = dwR1.detach(); U_[1] = dwR2.detach()\n",
    "                S_[0] = ds1.detach(); S_[1] = ds2.detach()\n",
    "                V_[0] = dwL1.detach(); V_[1] = dwL2.detach()\n",
    "            elif task >= 1:\n",
    "                U_[0] = torch.cat((U_[0], dwR1.detach()), dim = 1); U_[1] = torch.cat((U_[1], dwR2.detach()), dim = 1)\n",
    "                S_[0] = ds1; S_[1] = ds2\n",
    "                V_[0] = torch.cat((V_[0], dwL1.detach()), dim = 1); V_[1] = torch.cat((V_[1], dwL2.detach()), dim = 1)\n",
    "                \n",
    "            # Save selector matrices of every tasks:\n",
    "            S1_tasks.append(ds1); S2_tasks.append(ds2)\n",
    "            b_tasks.append([b1.detach(), b2.detach(), b3.detach()])\n",
    "            w3_tasks.append(w3.detach())\n",
    "            \n",
    "        # Plot loss:\n",
    "        plt.figure()\n",
    "        plt.plot(loss_epoch)\n",
    "        plt.title(['Subset:',str(fold),' Task:',str(task)])\n",
    "        plt.show()\n",
    "        # Plot epoch accuracy:\n",
    "        plt.figure()\n",
    "        plt.plot(epoch_accuracy)\n",
    "        plt.title(['Epochs accuracy:'])\n",
    "        plt.show()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        w_task[task][0]=dwR1.detach()\n",
    "        w_task[task][1]=dwL1.detach()\n",
    "        w_task[task][2]=dwR2.detach()\n",
    "        w_task[task][3]=dwL2.detach()\n",
    "        w_task[task][4]=w3.detach()\n",
    "        w_task[task][5]=ds1.detach()\n",
    "        w_task[task][6]=ds2.detach()\n",
    "        w_task[task][7]=b1.detach()  \n",
    "        w_task[task][8]=b2.detach()  \n",
    "        w_task[task][9]=b3.detach()\n",
    "\n",
    "    # Training accuracy:\n",
    "    train_accuracy=[]\n",
    "    for task in range(num_task):\n",
    "        \n",
    "        train_correct, train_total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            train_size=len(X_train[task][fold])\n",
    "            batch_no=np.int16(np.ceil(train_size/np.float64(batch_size)))\n",
    "        \n",
    "            epoch_idx=np.arange(train_size)\n",
    "            np.random.shuffle(epoch_idx)\n",
    "            \n",
    "            for batch_idx in range(0, batch_no):\n",
    "                x_batch=X_train[task][fold][epoch_idx[batch_idx*batch_size:np.min([(batch_idx+1)*batch_size,train_size])],:]\n",
    "                y_batch=Y_train[task][fold][epoch_idx[batch_idx*batch_size:np.min([(batch_idx+1)*batch_size,train_size])]]\n",
    "            \n",
    "                x_batch_tensor=torch.FloatTensor(x_batch).to(device)\n",
    "                y_batch_tensor=torch.FloatTensor(y_batch).to(device)\n",
    "            \n",
    "\n",
    "                u_1 = U_[0][:,0:r1+(task)*r2]\n",
    "                s_1 = S1_tasks[task]\n",
    "                v_1 = V_[0][:,0:r1+(task)*r2]\n",
    "                u_2 = U_[1][:,0:r1+(task)*r2]\n",
    "                s_2 = S2_tasks[task]\n",
    "                v_2 = V_[1][:,0:r1+(task)*r2]\n",
    "                    \n",
    "                w1=torch.matmul(torch.matmul(u_1, torch.diag(s_1)),v_1.T)\n",
    "                w2=torch.matmul(torch.matmul(u_2, torch.diag(s_2)),v_2.T)\n",
    "                    \n",
    "                h1=relu(torch.matmul(x_batch_tensor,w1) + b_tasks[task][0])\n",
    "            \n",
    "                h2=relu(torch.matmul(h1,w2) + b_tasks[task][1])\n",
    "                \n",
    "                # Prediction\n",
    "                w3 = w3_tasks[task]\n",
    "                y_hat=softmax(torch.matmul(h2,w3) + b_tasks[task][2]) # Multi-class\n",
    "                \n",
    "                prediction = np.argmax(y_hat.cpu().detach().numpy(),axis=1) # Multi-class\n",
    "                y_label=np.argmax(y_batch_tensor.cpu().detach().numpy(),axis=1) # Multi-class\n",
    "                train_correct += int(sum(y_label == prediction)) # Multi-class\n",
    "                train_total += int(len(y_label)) # Multi-class\n",
    "    \n",
    "        train_accuracy.append(float(train_correct / train_total))\n",
    "    print(\"Training Accuracy: \", train_accuracy, \"Avg: \", np.mean(train_accuracy))\n",
    "        \n",
    "    # Test accuracy:\n",
    "    task_accuracy=[]\n",
    "    for task in range(num_task):\n",
    "        \n",
    "        task_correct, task_total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            test_size=len(X_test[task][fold])\n",
    "            batch_no=np.int16(np.ceil(test_size/np.float64(batch_size)))\n",
    "        \n",
    "            epoch_idx=np.arange(test_size)\n",
    "            np.random.shuffle(epoch_idx)\n",
    "            \n",
    "            for batch_idx in range(0, batch_no):\n",
    "                x_batch=X_test[task][fold][epoch_idx[batch_idx*batch_size:np.min([(batch_idx+1)*batch_size,test_size])],:]\n",
    "                y_batch=Y_test[task][fold][epoch_idx[batch_idx*batch_size:np.min([(batch_idx+1)*batch_size,test_size])]]\n",
    "            \n",
    "                x_batch_tensor=torch.FloatTensor(x_batch).to(device)\n",
    "                y_batch_tensor=torch.FloatTensor(y_batch).to(device)\n",
    "\n",
    "                u_1 = U_[0][:,0:r1+(task)*r2]\n",
    "                s_1 = S1_tasks[task]\n",
    "                v_1 = V_[0][:,0:r1+(task)*r2]\n",
    "                u_2 = U_[1][:,0:r1+(task)*r2]\n",
    "                s_2 = S2_tasks[task]\n",
    "                v_2 = V_[1][:,0:r1+(task)*r2]\n",
    "                w1=torch.matmul(torch.matmul(u_1, torch.diag(s_1)),v_1.T)\n",
    "                w2=torch.matmul(torch.matmul(u_2, torch.diag(s_2)),v_2.T)\n",
    "                \n",
    "                    \n",
    "                h1=relu(torch.matmul(x_batch_tensor,w1) + b_tasks[task][0])\n",
    "                \n",
    "                h2=relu(torch.matmul(h1,w2) + b_tasks[task][1])\n",
    "                \n",
    "                # Prediction\n",
    "                w3 = w3_tasks[task]\n",
    "                y_hat=softmax(torch.matmul(h2,w3) + b_tasks[task][2])\n",
    "                prediction = np.argmax(y_hat.cpu().detach().numpy(),axis=1)\n",
    "                y_label=np.argmax(y_batch_tensor.cpu().detach().numpy(),axis=1)\n",
    "                task_correct += int(sum(y_label == prediction))\n",
    "                task_total += int(len(y_label))\n",
    "        \n",
    "        task_accuracy.append(float(task_correct / task_total))\n",
    "        \n",
    "    accuracy.append(task_accuracy)\n",
    "    fold_accuracy.append(task_accuracy)\n",
    "    print(\"Subset\", fold, \"Complete. Validation Accuracy: \", fold_accuracy, \"Avg: \", np.mean(fold_accuracy))\n",
    "    \n",
    "    \n",
    "disp_1 = [U_, S_, V_, S1_tasks, S2_tasks, b_tasks, w3_tasks]\n",
    "del U_, S_, V_, S1_tasks, S2_tasks, b_tasks, w3_tasks\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
