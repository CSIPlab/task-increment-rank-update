{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a90274a",
   "metadata": {},
   "source": [
    "# Loading necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c112bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isdir\n",
    "from os import mkdir\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ac9315",
   "metadata": {},
   "source": [
    "# Setting device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0519a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id=0\n",
    "torch.cuda.set_device(device_id)\n",
    "device='cuda:'+str(device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9268e881",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1fdb3c",
   "metadata": {},
   "source": [
    "## Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958265d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath='./datasets'\n",
    "if not isdir(datapath):\n",
    "    mkdir(datapath)\n",
    "\n",
    "mnist_train=datasets.MNIST(datapath, train= True, download = True)\n",
    "x_train=mnist_train.data\n",
    "y_train=mnist_train.targets\n",
    "mnist_test=datasets.MNIST(datapath, train= False, download = True)\n",
    "x_test=mnist_test.data\n",
    "y_test=mnist_test.targets\n",
    "\n",
    "x_train=x_train.reshape(x_train.shape[0],-1)\n",
    "x_test=x_test.reshape(x_test.shape[0],-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f5fba3",
   "metadata": {},
   "source": [
    "## Create Permuted MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94b4ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_task=20\n",
    "step=100\n",
    "init=100\n",
    "\n",
    "seeds=np.arange(init,num_task*step+init,step)\n",
    "n_class=len(np.unique(y_train))\n",
    "train_y=np.zeros((len(y_train),n_class))\n",
    "for i in range(train_y.shape[0]):\n",
    "    train_y[i,y_train[i]]=1\n",
    "test_y=np.zeros((len(y_test),n_class))\n",
    "for i in range(test_y.shape[0]):\n",
    "    test_y[i,y_test[i]]=1\n",
    "    \n",
    "data=[[x_train,train_y]]\n",
    "data_test=[[x_test,test_y]]\n",
    "\n",
    "pixels=np.arange(x_train.shape[1])\n",
    "\n",
    "pixel_set=[pixels]\n",
    "for task in range(num_task-1):\n",
    "    np.random.seed(seeds[task])\n",
    "    np.random.shuffle(pixels)\n",
    "    pixel_set.append(pixels)\n",
    "    \n",
    "    \n",
    "    x=x_train[:,pixels]    \n",
    "    data.append([x,train_y])\n",
    "    \n",
    "    x=x_test[:,pixels]    \n",
    "    data_test.append([x,test_y])\n",
    "    \n",
    "X_train, X_test = [], []\n",
    "Y_train, Y_test = [], []\n",
    "\n",
    "\n",
    "for task in range(num_task):\n",
    "    X_currTask_train=data[task][0].numpy()\n",
    "    Y_currTask_train=data[task][1]\n",
    "    X_currTask_test=data_test[task][0].numpy()\n",
    "    Y_currTask_test=data_test[task][1]\n",
    "\n",
    "    X_train.append(X_currTask_train) \n",
    "    X_test.append(X_currTask_test) \n",
    "    Y_train.append(Y_currTask_train)\n",
    "    Y_test.append(Y_currTask_test) \n",
    "X_train = np.asarray(X_train)\n",
    "X_train = np.reshape(X_train,(num_task,X_train.shape[1],X_train.shape[2])) \n",
    "X_test = np.asarray(X_test)\n",
    "X_test = np.reshape(X_test,(num_task,X_test.shape[1],X_test.shape[2])) \n",
    "Y_train = np.asarray(Y_train)\n",
    "Y_train = np.reshape(Y_train,(num_task,Y_train.shape[1],Y_train.shape[2])) \n",
    "Y_test = np.asarray(Y_test)\n",
    "Y_test = np.reshape(Y_test,(num_task,Y_test.shape[1],Y_test.shape[2])) \n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e5763a",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4280fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "num_task=20\n",
    "n_class=10 # Ouput dimension\n",
    "\n",
    "# Selection of r1, r2:\n",
    "r1= 11\n",
    "r2= 1\n",
    "\n",
    "batch_size=128\n",
    "# Epochs by task (task1, task2, task3 ....):\n",
    "train_epochs=[3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3]\n",
    "# train_epochs=[500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500,500]\n",
    "\n",
    "lr_base = 1e-5\n",
    "lr_cont = 1e-4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6100dfd",
   "metadata": {},
   "source": [
    "# Network (3 layer MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e7ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers def:\n",
    "relu = torch.nn.ReLU()\n",
    "# sigmoid=torch.nn.Sigmoid()\n",
    "softmax=torch.nn.Softmax(dim=1)\n",
    "mse_loss = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "w_task=[]\n",
    "for task in range(num_task):\n",
    "\n",
    "    if task==0:\n",
    "        r=r1\n",
    "        lr = lr_base\n",
    "    else:\n",
    "        r=r2\n",
    "        lr = lr_cont\n",
    "\n",
    "\n",
    "    # Initialize weights\n",
    "    #Layer 1\n",
    "    wR1 = torch.empty(x_train.shape[1], r).to(device)\n",
    "    wL1 = torch.empty(256, r).to(device)\n",
    "    s1 = torch.ones(r).to(device)\n",
    "    b1 = torch.zeros(256).to(device)\n",
    "    #Layer 2\n",
    "    wR2 = torch.empty(256, r).to(device)\n",
    "    wL2 = torch.empty(256, r).to(device)\n",
    "    s2 = torch.ones(r).to(device)\n",
    "    b2 = torch.zeros(256).to(device)\n",
    "    #layer 3\n",
    "    w3=torch.empty(256, n_class).to(device)\n",
    "    b3 = torch.zeros(n_class).to(device)\n",
    "\n",
    "    # Initialization:\n",
    "    torch.nn.init.orthogonal_(wR1)\n",
    "    torch.nn.init.orthogonal_(wL1)\n",
    "    torch.nn.init.orthogonal_(wR2)\n",
    "    torch.nn.init.orthogonal_(wL2)\n",
    "    torch.nn.init.orthogonal_(w3)\n",
    "\n",
    "    w_task.append([wR1,wL1,wR2,wL2,w3,s1,s2,b1,b2,b3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2dd7f2",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fdeff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train-Test sequence:\n",
    "print(\"Task Progress:\")\n",
    "# Train\n",
    "U_, S_, V_ = [],[],[]\n",
    "U_.append(w_task[0][0]); U_.append(w_task[0][2]) # U_[n], n-th interm layer.\n",
    "S_.append(w_task[0][5]); S_.append(w_task[0][6]) # s0\n",
    "V_.append(w_task[0][1]); V_.append(w_task[0][3]) # wL0\n",
    "S1_tasks = []\n",
    "S2_tasks = []\n",
    "b_tasks = []\n",
    "w3_tasks = []\n",
    "for task in tqdm(range(num_task)):\n",
    "\n",
    "    dwR1=torch.autograd.Variable(w_task[task][0],requires_grad=True)\n",
    "    dwL1=torch.autograd.Variable(w_task[task][1],requires_grad=True)\n",
    "    dwR2=torch.autograd.Variable(w_task[task][2],requires_grad=True)\n",
    "    dwL2=torch.autograd.Variable(w_task[task][3],requires_grad=True)    \n",
    "    w3=torch.autograd.Variable(w_task[task][4],requires_grad=True)\n",
    "    # Selector\n",
    "    if task == 0:\n",
    "        ds1 = torch.autograd.Variable(w_task[task][5],requires_grad=True)\n",
    "        ds2 = torch.autograd.Variable(w_task[task][6],requires_grad=True)  \n",
    "    else:\n",
    "        ds1 = torch.cat((torch.zeros(S1_tasks[task-1].shape).to(device), w_task[task][5]), dim = 0) \n",
    "        ds2 = torch.cat((torch.zeros(S2_tasks[task-1].shape).to(device), w_task[task][6]), dim = 0)      \n",
    "        ds1=torch.autograd.Variable(ds1,requires_grad=True)\n",
    "        ds2=torch.autograd.Variable(ds2,requires_grad=True)\n",
    "    # Bias\n",
    "    b1=torch.autograd.Variable(w_task[task][7],requires_grad=True)\n",
    "    b2=torch.autograd.Variable(w_task[task][8],requires_grad=True)\n",
    "    b3=torch.autograd.Variable(w_task[task][9],requires_grad=True)\n",
    "\n",
    "    optimizer=torch.optim.Adam([dwR1,dwL1,dwR2,dwL2,w3,ds1,ds2,b1,b2,b3],lr=lr)\n",
    "    #optimizer=torch.optim.SGD([dwR1,dwL1,dwR2,dwL2,w3,ds1,ds2,b1,b2,b3],lr=lr,momentum = 0.9)\n",
    "\n",
    "    loss_epoch=[]\n",
    "\n",
    "    epoch_correct, epoch_total = 0, 0\n",
    "    epoch_accuracy = []\n",
    "    times = 1\n",
    "    for epoch in tqdm(range(train_epochs[task])):\n",
    "        train_size=len(X_train[task])\n",
    "        batch_no=np.int16(np.ceil(train_size/np.float64(batch_size)))\n",
    "\n",
    "        epoch_idx=np.arange(train_size)\n",
    "        np.random.shuffle(epoch_idx)\n",
    "\n",
    "\n",
    "        loss_batch = 0\n",
    "        for batch_idx in range(0, batch_no):\n",
    "            x_batch=X_train[task][epoch_idx[batch_idx*batch_size:np.min([(batch_idx+1)*batch_size,train_size])],:]\n",
    "            y_batch=Y_train[task][epoch_idx[batch_idx*batch_size:np.min([(batch_idx+1)*batch_size,train_size])]]\n",
    "\n",
    "            x_batch_tensor=torch.FloatTensor(x_batch).to(device)\n",
    "            y_batch_tensor=torch.FloatTensor(y_batch).to(device)\n",
    "\n",
    "            if task == 0:\n",
    "                w1=torch.matmul(torch.matmul(dwR1,torch.diag(ds1)),dwL1.T)\n",
    "                w2=torch.matmul(torch.matmul(dwR2,torch.diag(ds2)),dwL2.T)\n",
    "            else:\n",
    "                U_inc1 = torch.cat((U_[0].to(device), dwR1), dim = 1)\n",
    "                V_inc1 = torch.cat((V_[0].to(device), dwL1), dim = 1)\n",
    "                w1=torch.matmul(torch.matmul(U_inc1,torch.diag(ds1)),V_inc1.T)\n",
    "\n",
    "                U_inc2 = torch.cat((U_[1].to(device), dwR2), dim = 1)\n",
    "                V_inc2 = torch.cat((V_[1].to(device), dwL2), dim = 1)\n",
    "                w2=torch.matmul(torch.matmul(U_inc2,torch.diag(ds2)),V_inc2.T)\n",
    "\n",
    "            h1=relu(torch.matmul(x_batch_tensor,w1)+b1)\n",
    "            h2=relu(torch.matmul(h1,w2)+b2)\n",
    "\n",
    "            y_hat=softmax(torch.matmul(h2,w3)+b3)\n",
    "\n",
    "            loss=mse_loss(y_hat,y_batch_tensor)\n",
    "\n",
    "            loss_batch = loss_batch + loss.item()\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Prediction:\n",
    "            prediction = np.argmax(y_hat.cpu().detach().numpy(),axis=1)\n",
    "            y_label=np.argmax(y_batch_tensor.cpu().detach().numpy(),axis=1)\n",
    "            epoch_correct += int(sum(y_label == prediction))\n",
    "            epoch_total += int(len(y_label))\n",
    "\n",
    "        epoch_accuracy.append(float(epoch_correct / epoch_total))\n",
    "        epoch_correct, epoch_total = 0, 0\n",
    "\n",
    "        loss_epoch.append(loss_batch)\n",
    "\n",
    "    # Concatenate the weight matrix of every tasks:\n",
    "    with torch.no_grad():\n",
    "        if task == 0:\n",
    "            U_[0] = dwR1.detach(); U_[1] = dwR2.detach()\n",
    "            S_[0] = ds1.detach(); S_[1] = ds2.detach()\n",
    "            V_[0] = dwL1.detach(); V_[1] = dwL2.detach()\n",
    "        elif task >= 1:\n",
    "            U_[0] = torch.cat((U_[0], dwR1.detach()), dim = 1); U_[1] = torch.cat((U_[1], dwR2.detach()), dim = 1)\n",
    "            S_[0] = ds1; S_[1] = ds2\n",
    "            V_[0] = torch.cat((V_[0], dwL1.detach()), dim = 1); V_[1] = torch.cat((V_[1], dwL2.detach()), dim = 1)\n",
    "\n",
    "        # Save selector matrices of every tasks:\n",
    "        S1_tasks.append(ds1); S2_tasks.append(ds2)\n",
    "        b_tasks.append([b1.detach(), b2.detach(), b3.detach()])\n",
    "        w3_tasks.append(w3.detach())\n",
    "\n",
    "    # Plot loss:\n",
    "    plt.figure()\n",
    "    plt.plot(loss_epoch)\n",
    "    plt.title([' Task:',str(task)])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Training loss')\n",
    "    plt.show()\n",
    "    # Plot epoch accuracy:\n",
    "    plt.figure()\n",
    "    plt.plot(epoch_accuracy)\n",
    "    plt.title([' Task:',str(task)])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Training accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w_task[task][0]=dwR1.detach()\n",
    "        w_task[task][1]=dwL1.detach()\n",
    "        w_task[task][2]=dwR2.detach()\n",
    "        w_task[task][3]=dwL2.detach()\n",
    "        w_task[task][4]=w3.detach()\n",
    "        w_task[task][5]=ds1.detach()\n",
    "        w_task[task][6]=ds2.detach()\n",
    "        w_task[task][7]=b1.detach()  \n",
    "        w_task[task][8]=b2.detach()  \n",
    "        w_task[task][9]=b3.detach()\n",
    "\n",
    "\n",
    "final_weights = [U_, V_, S1_tasks, S2_tasks, b_tasks, w3_tasks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de3e4a1",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06bd8ec",
   "metadata": {},
   "source": [
    "## On the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1be498",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_accuracy=[]\n",
    "for task in range(num_task):\n",
    "\n",
    "    test_correct, test_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        test_size=len(X_test[task])\n",
    "        batch_no=np.int16(np.ceil(test_size/np.float64(batch_size)))\n",
    "\n",
    "        epoch_idx=np.arange(test_size)\n",
    "        np.random.shuffle(epoch_idx)\n",
    "\n",
    "        for batch_idx in range(0, batch_no):\n",
    "            x_batch=X_test[task][epoch_idx[batch_idx*batch_size:np.min([(batch_idx+1)*batch_size,test_size])],:]\n",
    "            y_batch=Y_test[task][epoch_idx[batch_idx*batch_size:np.min([(batch_idx+1)*batch_size,test_size])]]\n",
    "\n",
    "            x_batch_tensor=torch.FloatTensor(x_batch).to(device)\n",
    "            y_batch_tensor=torch.FloatTensor(y_batch).to(device)\n",
    "\n",
    "            u_1 = U_[0][:,0:r1+(task)*r2]\n",
    "            s_1 = S1_tasks[task]\n",
    "            v_1 = V_[0][:,0:r1+(task)*r2]\n",
    "            u_2 = U_[1][:,0:r1+(task)*r2]\n",
    "            s_2 = S2_tasks[task]\n",
    "            v_2 = V_[1][:,0:r1+(task)*r2]\n",
    "            w1=torch.matmul(torch.matmul(u_1, torch.diag(s_1)),v_1.T)\n",
    "            w2=torch.matmul(torch.matmul(u_2, torch.diag(s_2)),v_2.T)\n",
    "\n",
    "\n",
    "            h1=relu(torch.matmul(x_batch_tensor,w1) + b_tasks[task][0])\n",
    "\n",
    "            h2=relu(torch.matmul(h1,w2) + b_tasks[task][1])\n",
    "\n",
    "            # Prediction\n",
    "            w3 = w3_tasks[task]\n",
    "            y_hat=softmax(torch.matmul(h2,w3) + b_tasks[task][2])\n",
    "            prediction = np.argmax(y_hat.cpu().detach().numpy(),axis=1)\n",
    "            y_label=np.argmax(y_batch_tensor.cpu().detach().numpy(),axis=1)\n",
    "            test_correct += int(sum(y_label == prediction))\n",
    "            test_total += int(len(y_label))\n",
    "\n",
    "    test_accuracy.append(float(test_correct / test_total))\n",
    "\n",
    "\n",
    "print(\"Test Accuracy: \", test_accuracy, \"Avg: \", np.mean(test_accuracy))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429cad81",
   "metadata": {},
   "source": [
    "## On the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad8c9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_accuracy=[]\n",
    "for task in range(num_task):\n",
    "\n",
    "    train_correct, train_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        train_size=len(X_train[task])\n",
    "        batch_no=np.int16(np.ceil(train_size/np.float64(batch_size)))\n",
    "\n",
    "        epoch_idx=np.arange(train_size)\n",
    "        np.random.shuffle(epoch_idx)\n",
    "\n",
    "        for batch_idx in range(0, batch_no):\n",
    "            x_batch=X_train[task][epoch_idx[batch_idx*batch_size:np.min([(batch_idx+1)*batch_size,train_size])],:]\n",
    "            y_batch=Y_train[task][epoch_idx[batch_idx*batch_size:np.min([(batch_idx+1)*batch_size,train_size])]]\n",
    "\n",
    "            x_batch_tensor=torch.FloatTensor(x_batch).to(device)\n",
    "            y_batch_tensor=torch.FloatTensor(y_batch).to(device)\n",
    "\n",
    "\n",
    "            u_1 = U_[0][:,0:r1+(task)*r2]\n",
    "            s_1 = S1_tasks[task]\n",
    "            v_1 = V_[0][:,0:r1+(task)*r2]\n",
    "            u_2 = U_[1][:,0:r1+(task)*r2]\n",
    "            s_2 = S2_tasks[task]\n",
    "            v_2 = V_[1][:,0:r1+(task)*r2]\n",
    "\n",
    "            w1=torch.matmul(torch.matmul(u_1, torch.diag(s_1)),v_1.T)\n",
    "            w2=torch.matmul(torch.matmul(u_2, torch.diag(s_2)),v_2.T)\n",
    "\n",
    "            h1=relu(torch.matmul(x_batch_tensor,w1) + b_tasks[task][0])\n",
    "\n",
    "            h2=relu(torch.matmul(h1,w2) + b_tasks[task][1])\n",
    "\n",
    "            # Prediction\n",
    "            w3 = w3_tasks[task]\n",
    "            y_hat=softmax(torch.matmul(h2,w3) + b_tasks[task][2]) # Multi-class\n",
    "\n",
    "            prediction = np.argmax(y_hat.cpu().detach().numpy(),axis=1) # Multi-class\n",
    "            y_label=np.argmax(y_batch_tensor.cpu().detach().numpy(),axis=1) # Multi-class\n",
    "            train_correct += int(sum(y_label == prediction)) # Multi-class\n",
    "            train_total += int(len(y_label)) # Multi-class\n",
    "\n",
    "    train_accuracy.append(float(train_correct / train_total))\n",
    "print(\"Training Accuracy: \", train_accuracy, \"Avg: \", np.mean(train_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b48f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
